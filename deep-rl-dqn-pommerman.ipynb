{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "deep-rl-dqn-pommerman.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-wallner/deep-rl-dqn-pommerman/blob/main/deep-rl-dqn-pommerman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRSq6sSTqZYK"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrSDf_h0B9C0"
      },
      "source": [
        "### Download and install env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfFgW911B09t"
      },
      "source": [
        "!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install onnx==1.8.1\n",
        "!pip install onnx2pytorch==0.2.0\n",
        "!pip install onnxruntime==1.2.0\n",
        "!pip install tqdm\n",
        "!pip install torchvision\n",
        "!pip install seaborn\n",
        "!pip install sklearn\n",
        "!pip install numpy\n",
        "!pip install opencv-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iIve1wBVfcz"
      },
      "source": [
        " # ## commands to install the env\n",
        "!git clone https://github.com/MultiAgentLearning/playground ./pommer_setup\n",
        "!pip install -U ./pommer_setup\n",
        "!rm -rf ./pommer_setup\n",
        "\n",
        "!git clone https://github.com/RLCommunity/graphic_pomme_env ./graphic_pomme_env\n",
        "!pip install -U ./graphic_pomme_env\n",
        "!rm -rf ./graphic_pomme_env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFQNjVpzB4NC"
      },
      "source": [
        "### Imports and Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWnfGIBWqpqu",
        "pycharm": {
          "is_executing": false
        }
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from functools import partial\n",
        "from itertools import chain\n",
        "import random\n",
        "from time import strftime\n",
        "from collections import deque, namedtuple\n",
        "from tqdm import trange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import gym\n",
        "from gym import Env, Wrapper\n",
        "from gym import logger as gymlogger\n",
        "\n",
        "from pommerman import make\n",
        "from pommerman.agents import BaseAgent, RandomAgent, SimpleAgent\n",
        "from graphic_pomme_env import graphic_pomme_env\n",
        "from graphic_pomme_env.wrappers import PommerEnvWrapperFrameSkip2\n",
        "\n",
        "import onnx\n",
        "from onnx2pytorch import ConvertModel\n",
        "\n",
        "import IPython.display\n",
        "from IPython.display import clear_output\n",
        "\n",
        "pomenvs = [es.id for es in gym.envs.registry.all() if es.id.startswith('Pomme')]\n",
        "print(\"\\n\".join(pomenvs))\n",
        "res = graphic_pomme_env.load_resources()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfgPUGNCXDSc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Define data paths\n",
        "data_path = '/content/gdrive/My Drive/Colab Notebooks/data/DRL/C4'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6AgRkzBW-Gu"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBnM8UQrXAlp"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQr5hKUUqdLG"
      },
      "source": [
        "### Helper Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9W9JwbUqfU_",
        "pycharm": {
          "is_executing": false
        }
      },
      "source": [
        "class Logger():\n",
        "  def __init__(self, logdir, params=None):\n",
        "      self.basepath = os.path.join(logdir, strftime(\"%Y-%m-%dT%H-%M-%S\"))\n",
        "      os.makedirs(self.basepath, exist_ok=True)\n",
        "      os.makedirs(self.log_dir, exist_ok=True)\n",
        "      if params is not None and os.path.exists(params):\n",
        "          shutil.copyfile(params, os.path.join(self.basepath, \"params.pkl\"))\n",
        "      self.log_dict = {}\n",
        "      self.dump_idx = {}\n",
        "\n",
        "  @property\n",
        "  def param_file(self):\n",
        "      return os.path.join(self.basepath, \"params.pkl\")\n",
        "\n",
        "  @property\n",
        "  def onnx_file(self):\n",
        "      return os.path.join(self.basepath, \"model.onnx\")\n",
        "\n",
        "  @property\n",
        "  def log_dir(self):\n",
        "      return os.path.join(self.basepath, \"logs\")\n",
        "\n",
        "  def log(self, name, value):\n",
        "      if name not in self.log_dict:\n",
        "          self.log_dict[name] = []\n",
        "          self.dump_idx[name] = -1\n",
        "      self.log_dict[name].append((len(self.log_dict[name]), time(), value))\n",
        "  \n",
        "  def get_values(self, name):\n",
        "      if name in self.log_dict:\n",
        "          return [x[2] for x in self.log_dict[name]]\n",
        "      return None\n",
        "  \n",
        "  def dump(self):\n",
        "      for name, rows in self.log_dict.items():\n",
        "          with open(os.path.join(self.log_dir, name + \".log\"), \"a\") as f:\n",
        "              for i, row in enumerate(rows):\n",
        "                  if i > self.dump_idx[name]:\n",
        "                      f.write(\",\".join([str(x) for x in row]) + \"\\n\")\n",
        "                      self.dump_idx[name] = i\n",
        "\n",
        "\n",
        "def plot_metrics(logger):\n",
        "  train_loss  = logger.get_values(\"training_loss\")\n",
        "  train_entropy  = logger.get_values(\"training_entropy\")\n",
        "  val_loss = logger.get_values(\"validation_loss\")\n",
        "  val_acc = logger.get_values(\"validation_accuracy\")\n",
        "  \n",
        "  fig = plt.figure(figsize=(15,5))\n",
        "  ax1 = fig.add_subplot(131, label=\"train\")\n",
        "  ax2 = fig.add_subplot(131, label=\"val\",frame_on=False)\n",
        "  ax4 = fig.add_subplot(132, label=\"entropy\")\n",
        "  ax3 = fig.add_subplot(133, label=\"acc\")\n",
        "\n",
        "  ax1.plot(train_loss, color=\"C0\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.set_xlabel(\"Update (Training)\", color=\"C0\")        \n",
        "  ax1.xaxis.grid(False)  \n",
        "  ax1.set_ylim((0,4))\n",
        "\n",
        "  ax2.plot(val_loss, color=\"C1\")\n",
        "  ax2.xaxis.tick_top()\n",
        "  ax2.yaxis.tick_right()\n",
        "  ax2.set_xlabel('Epoch (Validation)', color=\"C1\")     \n",
        "  ax2.xaxis.set_label_position('top')     \n",
        "  ax2.xaxis.grid(False)\n",
        "  ax2.get_yaxis().set_visible(False)\n",
        "  ax2.set_ylim((0,4))\n",
        "\n",
        "  ax4.plot(train_entropy, color=\"C3\")    \n",
        "  ax4.set_xlabel('Update (Training)', color=\"black\")     \n",
        "  ax4.set_ylabel(\"Entropy\", color=\"C3\")\n",
        "  ax4.tick_params(axis='x', colors=\"black\")\n",
        "  ax4.tick_params(axis='y', colors=\"black\")\n",
        "  ax4.xaxis.grid(False)    \n",
        "  # ax4.set_ylim((0,4))\n",
        "\n",
        "  ax3.plot(val_acc, color=\"C2\")\n",
        "  ax3.set_xlabel(\"Epoch (Validation)\", color=\"black\")\n",
        "  ax3.set_ylabel(\"Accuracy\", color=\"C2\")\n",
        "  ax3.tick_params(axis='x', colors=\"black\")\n",
        "  ax3.tick_params(axis='y', colors=\"black\")\n",
        "  ax3.xaxis.grid(False)\n",
        "  ax3.set_ylim((0,1))\n",
        "\n",
        "  fig.tight_layout(pad=2.0)\n",
        "  plt.show()\n",
        "\n",
        "def save_as_onnx(torch_model, sample_input, model_path):\n",
        "  torch.onnx.export(torch_model,               # model being run\n",
        "                    sample_input,              # model input (or a tuple for multiple inputs)\n",
        "                    f=model_path,              # where to save the model (can be a file or file-like object)\n",
        "                    export_params=True,        # store the trained parameter weights inside the model file\n",
        "                    opset_version=10,          # the ONNX version to export the model to - see https://github.com/microsoft/onnxruntime/blob/master/docs/Versioning.md\n",
        "                    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                    )\n",
        "def print_num_trainable_params(net):\n",
        "  num_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
        "  print(\"Trainable Parameters: {}\".format(num_trainable_params))\n",
        "\n",
        "def make_video(list_of_observations_of_a_player, prefix):\n",
        "    images = list_of_observations_of_a_player\n",
        "    height, width, layer = images[0].shape    \n",
        "    video_name = f'{prefix}-video.avi'\n",
        "    video = cv2.VideoWriter(video_name, 0, 3, (width,height))\n",
        "    for image in images:\n",
        "        video.write(image)\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "    video.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bY4ZsBLUjF4"
      },
      "source": [
        "### Some useful opponent actor functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2auqIc8MUjF5"
      },
      "source": [
        "def idle_actor(frame_stack):\n",
        "    del frame_stack\n",
        "    return 0\n",
        "  \n",
        "def random_actor(frame_stack):\n",
        "    del frame_stack\n",
        "    return np.random.randint(NUM_ACTIONS)\n",
        "\n",
        "def no_bomb_random_actor(frame_stack):\n",
        "    del frame_stack\n",
        "    return np.random.randint(NUM_ACTIONS-1)\n",
        "  \n",
        "def model_actor(frame_stack, model):\n",
        "    obs = torch.from_numpy(np.array(frame_stack.get_observation()))\n",
        "    net_out = model(obs).detach().cpu().numpy()\n",
        "    action = np.argmax(net_out)\n",
        "    return action\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RWfW7D7YDnG"
      },
      "source": [
        "### Prioritized replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY0adCl_YOUf"
      },
      "source": [
        "class ReplayBuffer():\n",
        "  def __init__(self, num_actions, memory_len = 10000):\n",
        "      self.memory_len = memory_len\n",
        "      self.transition = []\n",
        "      self.num_actions = num_actions\n",
        "\n",
        "  def add(self, state, action, reward, next_state, done):\n",
        "      if self.length() > self.memory_len:\n",
        "        self.remove()\n",
        "      self.transition.append(Transition(state, action, reward, next_state, done))\n",
        "\n",
        "  def sample_batch(self, batch_size = 32):\n",
        "      minibatch = random.sample(self.transition, batch_size)\n",
        "\n",
        "      states_mb, a_, reward_mb, next_states_mb, done_mb = map(np.array, zip(*minibatch))\n",
        "\n",
        "      mb_reward = torch.from_numpy(reward_mb).cuda()\n",
        "      mb_done = torch.from_numpy(done_mb.astype(int)).cuda()\n",
        "\n",
        "      a_mb = np.zeros((a_.size, self.num_actions))\n",
        "      a_mb[np.arange(a_.size), a_] = 1\n",
        "      mb_a = torch.from_numpy(a_mb).cuda()\n",
        "\n",
        "      return states_mb, mb_a, mb_reward, next_states_mb, mb_done\n",
        "\n",
        "  def length(self):\n",
        "      return len(self.transition)\n",
        "\n",
        "  def remove(self):\n",
        "      self.transition.pop(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAjITF_VYZs0"
      },
      "source": [
        "### DQN network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_VyMguHYg4P"
      },
      "source": [
        "class DQNNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, num_stack, num_actions):\n",
        "        super().__init__()\n",
        "\n",
        "        # CNN\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(num_stack, 32, 8, 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, 1),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "        \n",
        "        # FC head\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, num_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      # output forward should always be q values for all actions\n",
        "        #print(x)\n",
        "        #print(x.shape)\n",
        "        x = torch.tensor(x, dtype=torch.float).cuda()\n",
        "        if len(x.size()) == 3:\n",
        "          x = x.unsqueeze(dim=0)\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZGynlp6b1V6"
      },
      "source": [
        "### Update target network via soft update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKamnzHYcAFf"
      },
      "source": [
        "@torch.no_grad() # NO gradients!\n",
        "def soft_update(local_model, target_model, tau):\n",
        "    \"\"\"Soft update model parameters.\n",
        "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "    \"\"\"\n",
        "    local_params, target_params = local_model.parameters(), target_model.parameters()\n",
        "\n",
        "    for local, target in zip(local_params, target_params):\n",
        "        target *= (1 - tau) # in-place\n",
        "        target += tau * local # in-place"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d487nsvHcLV7"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hvUenJFcXOU"
      },
      "source": [
        "N_PLAYERS = 2 \n",
        "NUM_STACK = 5\n",
        "NUM_ACTIONS = 6\n",
        "'''\n",
        "0 Stop\n",
        "1 Up\n",
        "2 Down\n",
        "3 Left\n",
        "4 Right\n",
        "5 Bomb\n",
        "'''\n",
        "\n",
        "board = 'GraphicOVOCompact-v0'\n",
        "num_episodes = 40000 # number of episodes to run the algorithm\n",
        "buffer_size = 10 ** 5 * 3 # size of the buffer to use\n",
        "epsilon = 1.0 # initial probablity of selecting random action a, annealed over time\n",
        "timesteps = 0 # counter for number of frames\n",
        "minibatch_size = 128 # size of the minibatch sampled\n",
        "gamma = 0.99 # discount factor\n",
        "eval_episode = 100\n",
        "num_eval = 10\n",
        "tau = 1e-3 # hyperparameter for updating the target network\n",
        "learning_rate = 0.00001 \n",
        "update_after = 2000 # update after num time steps\n",
        "epsilon_decay = 10**5\n",
        "epsilon_ub = 1.0\n",
        "epsilon_lb = 0.02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VivC8YAc_Kz"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY1gpfZrdKHr"
      },
      "source": [
        "# Train the agent using DQN for Pommerman\n",
        "returns = []\n",
        "returns_500 = deque(maxlen=500)\n",
        "losses = []\n",
        "buffer = ReplayBuffer(num_actions=NUM_ACTIONS, memory_len=buffer_size)\n",
        "\n",
        "# Load pretrained model in PT format (or start from scratch)\n",
        "model_file = os.path.join(data_path, 'model_episode_19000.tar')\n",
        "dqn = DQNNetwork(num_stack=NUM_STACK, num_actions=NUM_ACTIONS).cuda()\n",
        "#dqn.load_state_dict(torch.load(model_file))\n",
        "\n",
        "dqn_target = DQNNetwork(num_stack=NUM_STACK, num_actions=NUM_ACTIONS).cuda()\n",
        "optimizer = optim.Adam(dqn.parameters(), lr=learning_rate)\n",
        "mse = torch.nn.MSELoss()\n",
        "\n",
        "checkpoint = torch.load(model_file)\n",
        "dqn.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "episode = checkpoint['episode']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "dqn.train()\n",
        "\n",
        "timesteps = 0\n",
        "for i in trange(episode, num_episodes):\n",
        "\n",
        "  # Set Pommerman gym environment with start_pos 0 or 1 randomly\n",
        "  if np.random.choice([0, 1], p=[0.5, 0.5]) == 0:\n",
        "    env = PommerEnvWrapperFrameSkip2(num_stack=NUM_STACK, start_pos=0, board=board)\n",
        "  else:\n",
        "    env = PommerEnvWrapperFrameSkip2(num_stack=NUM_STACK, start_pos=1, board=board)\n",
        "  state = env.reset()[0]\n",
        "  \n",
        "  ret = 0\n",
        "  done = False\n",
        "  while not done:\n",
        "    #print(state)\n",
        "\n",
        "    # Decay epsilon :\n",
        "    epsilon = max(epsilon_lb, epsilon_ub - timesteps/ epsilon_decay)\n",
        "    # action selection\n",
        "    if np.random.choice([0,1], p=[1-epsilon,epsilon]) == 1:\n",
        "      a = np.random.randint(low=0, high=NUM_ACTIONS, size=1)[0]\n",
        "    else:\n",
        "      net_out = dqn(state).detach().cpu().numpy()\n",
        "      a = np.argmax(net_out)\n",
        "    next_state, r, done, info = env.step(a)[0]\n",
        "    ret = ret + r\n",
        "\n",
        "    # Store transition in the replay buffer \n",
        "    buffer.add(state, a, r, next_state, done)\n",
        "\n",
        "    state = next_state\n",
        "    timesteps = timesteps + 1\n",
        "\n",
        "    # update policy using temporal difference\n",
        "    if buffer.length() > minibatch_size and buffer.length() > update_after:\n",
        "      optimizer.zero_grad()\n",
        "      # Sample a minibatch randomly\n",
        "\n",
        "      # Minibatch: Current states, advantage, rewards, next state, done\n",
        "      states_mb, mb_a, mb_reward, next_states_mb, mb_done = buffer.sample_batch(minibatch_size)\n",
        "\n",
        "      # Compute q values for states\n",
        "      q_values = dqn(states_mb)\n",
        "\n",
        "      # Compute the targets for training\n",
        "      q_values_next = dqn_target(next_states_mb)\n",
        "      targets = torch.where(\n",
        "          mb_done.byte(),\n",
        "          mb_reward.float(),\n",
        "          mb_reward.float() + gamma * torch.max(q_values_next, dim=-1).values\n",
        "      )\n",
        "\n",
        "      # Compute the predictions for training\n",
        "      predictions = q_values[mb_a != 0]\n",
        "\n",
        "      # Update loss: mse = mean squared error\n",
        "      loss = mse(predictions, targets)\n",
        "      loss.backward(retain_graph=False)\n",
        "      optimizer.step()\n",
        "      losses.append(loss.item())\n",
        " \n",
        "      # Update target network\n",
        "      soft_update(dqn, dqn_target, tau)\n",
        "    if done:\n",
        "      state_done = env.reset()\n",
        "      #print(torch.tensor(state_done, dtype=torch.float)[0].shape)\n",
        "      break\n",
        "  returns.append(ret)\n",
        "  returns_500.append(ret)\n",
        "  if i % 500 == 0 and i != 0:\n",
        "    wins_perc = sum(np.array(returns_500)==1) / len(returns_500) * 100\n",
        "    print(f'\\n\\nEpisode {i} \\t Wins: {wins_perc:.2f}%\\n')\n",
        "    dummy_input = torch.tensor(state, dtype=torch.float)\n",
        "    file_path = os.path.join(data_path, f'model_episode_{i}.onnx')\n",
        "    try:\n",
        "        save_as_onnx(dqn, dummy_input, file_path)\n",
        "        torch.save({\n",
        "            'episode': i,\n",
        "            'model_state_dict': dqn.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, os.path.join(data_path, f'model_episode_{i}.tar'))\n",
        "        \n",
        "    except:\n",
        "        '\\nError saving in ONNX format, saving in pytorch PT format instead.\\n'\n",
        "        #torch.save(dqn.state_dict(), os.path.join(data_path, f'model_episode_{i}.pt'))\n",
        "        torch.save({\n",
        "            'episode': i,\n",
        "            'model_state_dict': dqn.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, os.path.join(data_path, f'model_episode_{i}.tar'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9-IA6STe6f-"
      },
      "source": [
        "### Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRZ8En1Ne6f-"
      },
      "source": [
        "gymlogger.set_level(40)  # error only\n",
        "np.random.seed(147)\n",
        "torch.manual_seed(147)\n",
        "N_EPISODES = 100\n",
        "\n",
        "# Specify pretrained model in ONNX format\n",
        "model_file = os.path.join(data_path, f'model_episode_17500.onnx')\n",
        "\n",
        "# Network\n",
        "net = ConvertModel(onnx.load(model_file), experimental=True)\n",
        "net.eval()\n",
        "\n",
        "win_count = 0.0\n",
        "#env = PommerEnvWrapperFrameSkip2(num_stack=5, start_pos=0, board='GraphicOVOCompact-v0')\n",
        "\n",
        "for i in trange(N_EPISODES):\n",
        "\n",
        "    # Set Pommerman gym environment with start_pos 0 or 1 randomly\n",
        "    if np.random.choice([0, 1], p=[0.5, 0.5]) == 0:\n",
        "        env = PommerEnvWrapperFrameSkip2(num_stack=NUM_STACK, start_pos=0, board=board)\n",
        "    else:\n",
        "        env = PommerEnvWrapperFrameSkip2(num_stack=NUM_STACK, start_pos=1, board=board)\n",
        "\n",
        "    done = False\n",
        "    obs, opponent_obs = env.reset()\n",
        "    while not done:\n",
        "        obs = torch.from_numpy(np.array(obs)).float()\n",
        "        net_out = net(obs).detach().cpu().numpy()\n",
        "        action = np.argmax(net_out)\n",
        "\n",
        "        agent_step, opponent_step = env.step(action)\n",
        "        obs, r, done, info = agent_step\n",
        "\n",
        "    if r > 0:\n",
        "        win_count += 1\n",
        "\n",
        "print(win_count / N_EPISODES)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}